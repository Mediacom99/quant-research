The input data given for this task is comprised of:
\begin{enumerate}
    \item Five historical series of daily logarithmic returns pertaining to stock indices from five different geographic regions. (Identified throughout the report as 'Stock returns')
    \item Six historical series of Fundamental Indicators for each geographic area. ('Fundamentals')
    \item Two historical series of daily log returns for Rates of each geographic area. ('Rates returns')
    \item Two historical series of Macroeconomic indicators for each geographic area. ('Macro indices')
    \item Three historical series of daily log returns related to currency crossings. ('Forex returns')
    \item Three historical series of log returns for three different commodities. ('Commodities returns')
\end{enumerate}
The historical series span from 30th September of 2003 to 31st of December of 2019. Before running any type of cleaning process I applied a log transformation to the historical series of Macroeconomic indices and Fundamentals Indicators.


From a first look at the data it is obvious that some cleaning is required. There are invalid numeric values like zeroes for logarithmic returns and NaN (Not-a-Number). I decided to use a careful and simple approach to avoid introducing too much noise in the data. First I replaced every type of invalid value into a NaN, then I forward and backward filled the data to make sure each value in each Dataframe is a valid number (float64). 


At this point a pretty straightforward problem arises: forward and backward filling just copies data, either from after or before, leaving us with a lot of repeated values. To smooth out this values a rolling average of five days is applied only on the previously filled values.


This is the most simple approach in order to do the minimum required to clean the input data. Another way could be to just throw away every value that is not valid, but it is better to try and keep as much data as possible for consistency, especially since part of the data will be reserved for testing the model's performance.


More invasive approach can be used, like Winsorization and LOWESS (Locally Weighted Scatterplot Regression), but the risk of changing the data distribution too much without really moving towards a normal distribution is high.

During the data cleaning process the historical data was resampled into different timeframes. The resampling into daily data has been done right after the data cleaning process.

The module \textbf{clean\_data.py} is dedicated to cleaning the data. It will read raw data from a file called 'data.xlsx' and will output an xlsx file called 'formatted-data.xlsx' with the cleaned data.