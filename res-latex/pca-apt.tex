% Remember, OSL requires no colinearit√†, no varying variance, and most of the test Im going to do require normality.
% The neural network might be a better choice to also consider time-varying and non-linear multi-factor model
% Model training: 
% \begin{enumerate}
%     \item Principal Component Analysis on the factors for reducing number of factors and removing the colinearity problem that might break OSL. I should also perform PCA both between all assets and between assets of same category (forex, commodities, rates, fundamentals, macro).
%     \item Arbitrage Pricing Theory + Linear Regression of returns against factors (multifactor-model with linear regression)
%     \item Long Short-Term Memory neural network + Layer-wise Relevance Propagation (LRP) for interpreting results, as shown in \cite{nakagawa}
% \end{enumerate}

% What are the assumptions of PCA + APT + OSL ? Does the data I have make sense ?
% Of course I can say its gonna be better with the neural network.

% Test model performance against out of sample data. Calculate relevant numbers to understand performance. 

% Write what model are you using to describe the returns and their relationship with the factors. Also describe PCA on factors, how does it work ? Then explain how you proceeded in practice: splitting datasets, what algorithm used, check for normality and other stuff on residuals. If residuals are bad I can change regression algorithm.
% If I want I can also change PCA methods. Or apply PCA on every factors or just between assets factors or dividing each factor based on the country.

% Instead of trying to understand the complexity and time-variance of stocks returns by fitting a very weird and complicated curve on the data, it could make sense to trade 'locally' on that curve, by locally I mean trading in a small enought time scale such that it could make sense to approximate the relations between factors and stocks as linear. That is why this model needs to have a frequent rebalancing history. (Maybe it could be nice to implement using different training data, like more modern or older, and using weights as a mean of those ones).

% REMEMBER: if osl does not work maybe I can use data in a smaller time frame, like last 2-3 years or 1 year and use rolling window.


This section describes the process of implementing and training a multi-factor model in conjunction with Principal Component Analysis on the factors for forecasting expected returns and volatility to later use for the portfolio optimization.

First it's important to establish the goal of this model: finding a relationship between stock returns and all the other historical daily data in the dataset, which we will call factors (rates, currency crossings, commodities, macro indices, fundamentals of stock), such that it's possible to use this relationship to forecast future expected stock returns knowing the values of the factor at the current time.
Three main questions immediately come to mind: \textbf{what distribution should one use to model stock returns as random variables ? What kind of mathematical relationship is there between the factors and the stock returns ? How should one choose which factors to use ?} 

The first question requires an assumption, usually it is required that either the actual returns or the log returns follow a normal distribution. As shown in the previous section, the stock log returns are 'decently' close to being described by a normal distribution if not for the presence of clustering near the center and not normal tails. For this task we will proceed as if the stock returns (as logarithmic daily data) follow a univariate gaussian distribution so that the vector of stocks follows a multivariate normal distribution:
\begin{equation}
    \Vec{r} = N(\Vec{\mu}, \Vec{\sigma})
\end{equation}
where: $\Vec{r} = (r_1, r_2, r_3, r_4, r_5)$ is the stock returns vector, $\Vec{\sigma} = (\sigma_1, \dots, \sigma_5)$ is the vector of stock returns' standard deviations.

The  portfolio return will follow this normal  distribution:
\begin{equation}
        r_p = N(\Vec{w}\cdot\Vec{\mu}, \Vec{w}^{T}\mathbf{\Sigma}\Vec{w})
\end{equation}
where: $\Vec{w}$ is the vector of weights which will define the portfolio allocation. The weights are the result of the optimization process. $\Sigma = (\sigma_{i,j})$ ($i,j = 1,...,5)$ is the covariance matrix of expected returns.
Normality is not really required for the multi-factor model but it is required when optimizing the portfolio using a risk-parity approach where all the information present in the objective function solely comes from the covariance matrix of the forecasted stock returns. This needs to be taken into account when interpreting both the model and the portfolio optimization results.
I COULD CITE THE TEXTBOOK I USED!


Now for the second question we need another assumption: the relationship between the stock returns and the chosen factors is \textbf{linear}, so:
\begin{equation}
    r_i = a_i + B_{1,i}F_{1,i} + ... + \epsilon_i
    \label{multifactor}
\end{equation}
where $\epsilon$ is the random error from the regression model used to determine the parameters in the above equation. The only thing an ordinary least squares linear regression model requires is the normality of $\epsilon$. For this reason we will show the performance of different linear regressor that are more robust and do not need the residuals to be normally distributed .Also it would help to have factor returns that are not correlated between each other, in order to lower the probability of overfitting.

Finally to tackle the problem of choosing the factors and at the same time dealing with correlation between factors I decided to use Principal Component Analysis on the factors in order to reduce their dimensionality by choosing only the first $K$ principal components such that a certain percentage of the total variance of the factors is explained, usually around $80-90\%$ is used.

Equation \eqref{multifactor} can be written in vector-matrix notation:
\begin{equation}
    \Vec{r}_{t+1} = \vec{b_0} + \mathbf{B}\vec{F}_{t} + \vec{\epsilon}
    \label{linear-multifactor-model}
\end{equation}
Where: $\mathbf{B} = (b_{i,j})$ is the matrix of factors exposures, $i$ runs over the number of stock returns (N) while $j$ runs over the number of factors (K) chosen for the regression model, thus it's a (N,K) matrix. The matrix of the factors exposures is the objective of the regression model. The vector $\vec{F}_{t}$ is a (1,K) vector which represents the factor at time t. The vector $\vec{r}_{t+1}$ represents the expected returns at time t+1. The vector $\vec{\epsilon}$ represents the vector of errors of the regression model.

Given this model, it can be shown that the covariance matrix of expected returns can be written as:
\begin{equation}
    \mathbf{\Sigma} = \mathbf{B}\mathbf{\Sigma}_{F}\mathbf{B}^{T} + \mathbf{S}
    \label{cov-matrix-returns}
\end{equation}
Where: $\mathbf{S}$ is a diagonal matrix consisting of each stocks' error $\epsilon$'s variance. $\mathbf{\Sigma}_{F}$ is the covariance matrix of the factors used in the regression model. Since we are using PCA, this matrix will always be diagonal.
It is very important to notice that the only place where time dependence of data is considered is in the regression model where the stock returns of time t+1 are calculated based on the factors at time t. There are more sophisticated methods to model the time-dependence of data, for example using multiple lagged features, not only the day before, or using more advanced time series models like ARIMA or GARCH for capturing complex temporal patterns.

There exists more complex models which incorporate both time-volatility and non-linear relations between stock returns and factors. For example in \cite{nakagawa} it is proposed the use of a Long Short-Term Memory Recurrent Deep Neural Network, combined with Layer-wise Relevant Propagation (LRP) in order to interpret the results. As they show in the table comparing different methods the LSTM + LRP model definitely outperforms the linear multi-factor model.

The idea is to use a linear regression model to calculate both $\mathbf{B}$ and $\mathbf{S}$. Then it is possible to obtain the covariance matrix of expected returns and use it to optimize the portfolio as shown in Section \ref{optimization}.

\subsection{Principal Component Analysis on Factors}
% calcoli pca e butto assieme al multifactor model. Also YUou could use LASSO with Ridge regression (Elastic Net) before PCA.
As we have seen in Figure \ref{fig:correlation-heatmap} the highest correlation is between Macro returns and Fundamentals. For this reason, and also to simplify the interpretation of the remaining principal components, I decided to use standard PCA between Macro returns and Fundamentals. With these principal components selected I will run standard PCA again against all the other remaining factors and the 'Macro-Fundamental' principal components I found before.
The standard Principal Components Analysis basically diagonalizes the covariance matrix and finds its eigenvectors. Using these eigenvectors as the transformation matrix it is possible to write every element of the initial base into the new base where the covariance matrix is diagonal (here the absence of covariance). One can then choose how many principal components to keep, thus how many of the eigenvectors with the biggest eigenvalues to keep. These principal components are directions in the new base that explain the most variance of the analyzed data. For this task I decided to choose the first five principal components, after some trial and error I found it was the most optimal choice, as it's shown in the following table and graphs. In reality once should carefully decide how to choose principal components, maybe even forcing a certain total explained variance, leaving the number of components chosen as variable.

The following results have been calculated using the whole dataset apart from the last year which was kept for testing purposes.
In Figure \ref{fig:factor-contribution-pca1} we can see the contribution of each Macroeconomic factor and Fundamentals factor to the first five principal components chosen. There are some peaks, suggesting that there are certain factors that prevail on the others but overall the distribution is pretty equal.
In Figure \ref{fig:factor-contribution-pca2} the situation is quite different: we can see that the first three final principal components are basically the same as the first three Macro and Fundamentals principal components; the last two final principal components are a combination of rates, commodities and forex.
These final five principal components will be used in the next section as the input data to train the linear model.
Table \ref{tab:pca-explained-variance} shows the percentage of the total variance of the factors explained by each principal component chosen, as well as the total explained variance of the first and second PCA run.

\begin{table}[htb]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|c|c|c|c|c|c|c|}
			\hline
			\textbf{}                           & \textbf{PC0} & \textbf{PC1} & \textbf{PC2} & \textbf{PC3} & \textbf{PC4} & \textbf{Total explained variance (\%)} \\ \hline
			\textbf{Macro-Fundamental PCA (\%)} & 42.31        & 22.66        & 14.04        & 4.66         & 3.74         & 87.40                                  \\ \hline
			\textbf{Final PCA (\%)}             & 33.23        & 17.79        & 11.02        & 5.58         & 4.53         & 72.15                                  \\ \hline
		\end{tabular}%
	}
	\caption{Percentages of explained variance for each principal component and the total variance for the first and second PCA run}
	\label{tab:pca-explained-variance}
\end{table} 

\begin{figure}[htb]
	\centering
	\includesvg[width=\textwidth]{Images/factor-contribution-to-pcaMacroFund1.svg}
	\caption{Macroeconomic and Fundamentals PCA factors loadings}
	\label{fig:factor-contribution-pca1}
\end{figure}

\begin{figure}[htb]
	\centering
	\includesvg[width=\textwidth]{Images/factor-contribution-to-pca2.svg}
	\caption{}
	\label{fig:factor-contribution-pca2}
\end{figure}

\subsection{K-Fold Cross Validation of Linear Regressors}
The principal components of the factors are the independent variables in the linear multi-factor model in eq.\eqref{linear-multifactor-model}. The next step is to decide which linear regression model and which kind of loss function to use.
First I decided to use a simple Ordinary Least Squares model (scikit-learn LinearRegression() model) in order to see the distribution of the residuals. I kept the last year of data out of the training sample for testing purposes.

In Figure \ref{fig:linreg-residuals} each stock index residual distribution's histogram is shown. We can immediately see the same problem we encountered in Figure \ref{fig:stock-returns-normal}, a clear grouping around the center of the distribution. We can confidently conclude that these residuals do not strictly follow a Gaussian distribution.

\begin{figure}[htb]
	\centering
	\includesvg[width=\textwidth]{Images/linear-regression-residuals-hist.svg}
	\caption{Gaussian fit for the residuals of each stock using Ordinary Least Squares}
	\label{fig:linreg-residuals}
\end{figure}

For this reason I decided to perform K-Fold Cross Validation on different kinds of more robust linear regression models and loss functions. This cross validation has been performed on the whole dataset apart from the last year that was kept for later testing. The splitting strategy used respects the time nature of the data because successive training sets are supersets of those that came before them.
The library uses the appropriate scoring method for each model and it normalizes different scores' results such that the highest number always represents the best score. [CITE THIS COMMENT]

%https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html#sklearn.model_selection.TimeSeriesSplit]
Here is the list of linear regression models from the python library \textit{scikit-learn} that I used to perform this cross validation:
\begin{itemize}
	\item LinearRegression()
	\item PassiveAggressiveRegressor()
	\item ElasticNet()
	\item HuberRegressor()
	\item TweedieRegressor()
	\item SGDRegressor(loss='huber', shuffle=False)
	\item SGDRegressor(loss='epsilon\_insensitive', shuffle=False)
	\item SGDRegressor(loss='squared\_epsilon\_insensitive', shuffle=False, penalty='elasticnet', epsilon = 0.001 * Y.std().mean())
\end{itemize}
Note: "()" means the regression model was called with the library default parameters.


In Table \ref{tab:crossval-results} the cross validation results are shown. It's clear that the Stochastic Gradient Descent Regressor with a squared epsilon insensitive loss function, L1 and L2 penalties (Elastic Net penalty) and epsilon chosen as 1\% of the mean of the volatility of each stock index return is the best performing regressor.

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Regressors}                     & \textbf{Mean Score} \\ \hline
		LinearRegression (OSL)                  & -18.29              \\ \hline
		Passive Aggressive Regressor            & -14.75              \\ \hline
		OSL + ElasticNet                        & -18.28              \\ \hline
		Huber Regressor                         & -19.40              \\ \hline
		Tweedie Regressor                       & -18.28              \\ \hline
		SGD + Huber                             & -12.26              \\ \hline
		SGD + Epsilon Insensitive               & -13.70              \\ \hline
		SGD + Squred Epsilon Ins. + Elastic Net & -5.06               \\ \hline
	\end{tabular}
	\caption{K-Fold timeseries cross validation results.}
	\label{tab:crossval-results}
\end{table}

EXPLAIN THE REASON behind this loss function and how the elastic net penalty works.
The Stochastic Gradiend Descent is a minimization algorithm that calculates with random sampling the gradient of the following function:
\begin{equation}
	Equazione di cui fai il gradiente
\end{equation}
Squared epsilon insensitive = cutoff at epsilon, then its quadratic

Using this method on the training data we can calculate the matrix of factor exposures B in \eqref{linear-multifactor-model} and use eq.\eqref{cov-matrix-returns} to calculate the covariance matrix of expected returns. With this matrix we can implement an optimization algorithm to find the best asset allocation for our portfolio.
All subsequent results have been calculated using the model with the highest score (the last one in the list)

